{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0114d7-c78a-4902-9a59-0bb377747f7f",
   "metadata": {},
   "source": [
    "# Classification of bird or airplane, by conv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d835b31-6561-4860-87fa-b1a35fd23d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a960d-b893-4ed5-bf8a-55c19f395c13",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We continue use the same cifar2 set which used in last fully connected model.\n",
    "And normalize data and remap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9436554d-e6f5-4464-8e48-9b7668d6d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n",
      "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "# load CIFAR10\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "# abstract birds and plane as CIFAR2\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd960510-cb6a-42cb-be62-be3e7688de10",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "The last model we used is a fully connected model. The model will remember the position of bird or airplane, so the model don't have generalization!\n",
    "\n",
    "At this time, we can use the convolution, it has 2 benefits:\n",
    "- convolution will calculate in who image, it will ignore the position of target.\n",
    "- convolution kernal will reduce the parameters in model than fully connected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a2fbd7-9cd0-4f58-bb8f-31d86d120cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create conv kernel, assume kernel in every channel(RBG) have same size.\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a2c96-76a6-43ac-86ad-1835046cbf1c",
   "metadata": {},
   "source": [
    "We find that the result of convolution kernel shrink the tensor size (because of kernel size 3, 3//2 = 1, missing 1 pixel on each side).\n",
    "For two resons we using padding to keep the result of convolution kernel's size: 1. the shrinked result will confuse us; 2. in resnet, keep in same size is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb9b467-ee9c-4b85-979a-6bee9a8cb21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 32, 32]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding to keep size.\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0017ba-2571-43f7-8d70-eeca28a6b063",
   "metadata": {},
   "source": [
    "Using padding technology, we can keep the size of tensor after convolution.\n",
    "Now we can use the Max pool to reduce the size of picture, to make model can 'see' more range, which provide layer to learn holistic characteristics of target.\n",
    "\n",
    "## Max pooling\n",
    "There are serval pooling methods, like average pooling or max pooling. And nowadays, researchers more like the max pooling, because of they can keep some features from the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f6725c-ae5a-4037-a139-57dcd4c69d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max pool\n",
    "pool = nn.MaxPool2d(2) # decimate in x2\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75027cf9-f9e2-429c-acde-dfb518db501b",
   "metadata": {},
   "source": [
    "## Construct new convolution model\n",
    "Now, we can use the conv kernel introducted before, to construct new model. We can just replace input layer of fully connected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8074f6-e8de-4fc8-b6d9-ab7a25c38817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # ... convert size\n",
    "            nn.Linear(8*8*8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b28f86-b731-4bab-a41c-a06de55653f4",
   "metadata": {},
   "source": [
    "Here we find in the above model, there is missing a gadget to 'flaten' convolution layer output to fit the linear layer input. So we introduce the nn.Module to upgrade the nn.Sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87bda7f-8cef-40ed-a765-66044e024e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeb559-d977-4703-a490-844ce2fe3657",
   "metadata": {},
   "source": [
    "Checkout our net defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48b2181-8a8e-4908-8aab-51c04e3d7b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1087, -0.0359]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d27856-e285-494b-9882-f6543ee5e578",
   "metadata": {},
   "source": [
    "## Training model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b14c5a20-378b-4489-8b82-d052434dea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f'{datetime.datetime.now()} Epoch {epoch}, Training loss {loss_train/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d21c1127-712c-4ba7-8053-33616353ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-24 15:23:58.789192 Epoch 1, Training loss 0.6127399350427518\n",
      "2024-02-24 15:24:20.832954 Epoch 10, Training loss 0.34056056029857346\n",
      "2024-02-24 15:24:45.468617 Epoch 20, Training loss 0.29784967081182323\n",
      "2024-02-24 15:25:10.152612 Epoch 30, Training loss 0.27266352139650635\n",
      "2024-02-24 15:25:34.774068 Epoch 40, Training loss 0.25076683719826354\n",
      "2024-02-24 15:25:59.407091 Epoch 50, Training loss 0.23074330688472006\n",
      "2024-02-24 15:26:24.190240 Epoch 60, Training loss 0.21134370423046647\n",
      "2024-02-24 15:26:48.894683 Epoch 70, Training loss 0.19650284015828637\n",
      "2024-02-24 15:27:13.608154 Epoch 80, Training loss 0.17896622082420216\n",
      "2024-02-24 15:27:38.331984 Epoch 90, Training loss 0.16655958785562758\n",
      "2024-02-24 15:28:03.086416 Epoch 100, Training loss 0.15171305562375456\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "model = Net()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(n_epochs=100, optimizer=opt, model=model, loss_fn=loss_fn, train_loader=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ec777-b059-4796-a278-49d0306b046a",
   "metadata": {},
   "source": [
    "## Validate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11c6461a-2fa4-4b81-b03d-ee66ae33f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just validate model on train and validate dataset\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # <1>\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
    "                total += labels.shape[0]  # <3>\n",
    "                correct += int((predicted == labels).sum())  # <4>\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2b7d9a0-b078-4f9d-91e0-ff35ce50cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.90\n",
      "Accuracy val: 0.84\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243896a-5022-4744-8bda-d6d21af8f4af",
   "metadata": {},
   "source": [
    "## Save and load trained model parameters\n",
    "\n",
    "```\n",
    "# save\n",
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')\n",
    "\n",
    "# load\n",
    "loaded_model = Net()  # <1>\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e68e89-d8c7-4bf0-90a8-3baa026b3cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchENV",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
